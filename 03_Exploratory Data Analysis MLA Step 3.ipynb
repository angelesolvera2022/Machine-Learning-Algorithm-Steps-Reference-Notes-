{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528e82e6-fce3-493a-928a-d8cc12ce7c48",
   "metadata": {},
   "source": [
    "1. Problem identification \n",
    "\n",
    "2. Data wrangling\n",
    "\n",
    "3. **Exploratory data analysis**\n",
    "\n",
    "4. Prep-processing and training data development \n",
    "\n",
    "5. Modeling (Machine learning steps)\n",
    "\n",
    "6. Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918e551-4560-4387-ba92-5f3feea3bef1",
   "metadata": {},
   "source": [
    "# Exploratory data analysis\n",
    "-  is an approach for summarizing and visualizing the important characteristics and statistical properties of a dataset. Visualizing the data will help you make sense of it to identify emerging themes. Identifying these trends will help you to form hypotheses about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab428c-14e8-404f-b6be-7380dcaa71b3",
   "metadata": {},
   "source": [
    "# **Data Validation** \n",
    "- It involves ensuring that the data is accurate, clean, and suitable for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a3d9b-f780-4dfc-b8fd-b730d5424cfc",
   "metadata": {},
   "source": [
    "**1) update data types that need to be differest**\n",
    "\n",
    "- `data[\"column\"] = data[\"column\"].astype(int)`\n",
    "\n",
    "  \n",
    "Rule of thumb when theres a date or time column always convert it to pd.datetime so its not missrepresented as a string\n",
    "- `df['time'] = pd.to_datetime(df['time'])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b087d7d-aa49-4267-8a4c-d05a23062667",
   "metadata": {},
   "source": [
    "**2) validating categorical data**\n",
    "- write a code that goes down the column you need to validate that checks if the variable is or not what you are looking for. Store the series in a new variable. \n",
    "\n",
    "- ```python \n",
    "    variable = ~data[\"columntovalidate\"].isin([\"valuetofind\"])\n",
    "    data[variable]   \n",
    "    ```                            \n",
    "- The code will return True or False, in this context if it returns True then the value is not in the series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763cff58-0000-4b9a-af51-32648db7ef21",
   "metadata": {},
   "source": [
    "**3) Validating numerical data**\n",
    "- Ensures that the numerical values are correct and reliable.\n",
    "- You can validate numerical data by vizually identifying the range, quartiles, and potential outliers. This is also part of dealing with outliers\n",
    "\n",
    "- ```python\n",
    "  \n",
    "    print(unemployment[\"2021\"].min(), unemployment[\"2021\"].max())\n",
    "    sns.boxplot(x = \"column\", y = \"column\", data = data)\n",
    "    plt.show()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d3a61-b711-4b0e-a354-6e5893d1afe1",
   "metadata": {},
   "source": [
    "**4) Data summarization**\n",
    "- We can explore the characteristics of subsets of data further with the help of the .groupby() function. \n",
    "\n",
    "`data.groupby(\"column\").mean()`\n",
    "\n",
    "- you can also use .sum(), .max(), .min(). etc\n",
    "\n",
    "**When you want to do several aggregations on columns use the .agg()\n",
    "\n",
    "`data.agg([\"mean\", \"std\"])`\n",
    "- this does the aggregation on all the numeric columns \n",
    "\n",
    "**If you want to do different aggregations to different columns you can specify**\n",
    "\n",
    "- ```python \n",
    "      \n",
    "    data.groupby(\"column\").agg(\n",
    "    mean_rating = (\"column\", \"mean\"),\n",
    "    std_rating = (\"column\", \"std)\n",
    "    )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554be11-4f9a-47b1-b33d-c984f078bea1",
   "metadata": {},
   "source": [
    "**5) Validating Categorical summaries**\n",
    "- best thing to do is to create a barplot \n",
    "\n",
    "- ```python \n",
    "    sns.barplot(data = data, x = \"columncategory\", y = \"columnfrequency\")\n",
    "    plt.show()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9475ee4f-e343-47e3-bb67-cddc9829c625",
   "metadata": {},
   "source": [
    "# **Data Cleaning and Imputation**\n",
    "- Dealing with Missing Values: Validate the effectiveness of data wrangling and handle any remaining issues.\n",
    "\n",
    "- Visualize missing data patterns using heatmaps or other plots.\n",
    "\n",
    "- Ensure that the methods used for handling missing values in the wrangling step are appropriate.\n",
    "\n",
    "- If needed, further handle missing values based on insights gained during EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8dce83-29d6-4c91-840d-f98811f5d313",
   "metadata": {},
   "source": [
    "**1) Adressing missing data** \n",
    "\n",
    "- not dealing with the missing values makes the data less representative of the population. \n",
    "\n",
    "- Drop missing values if they amount to 5% or less of all values.\n",
    "\n",
    "- If theres more we can replace them for mean, median or mode (imputation) \n",
    "\n",
    "    **Method 1** Find the treshold: a value that indicates a limit. It helps determine if a decision should be done or not. Here the decision is should the missing values be dropped or not? Is treshold 5% or less or more than 5%. If ledd then drop all missing values if not then find a strategy to deal with them. \n",
    "    \n",
    "    \n",
    "    \n",
    "-  `data.isna.().sum()`\n",
    "- `treshold = len(data) * 0.05`\n",
    "- `cols_to_drop = data.columns[ data.isna().sum()<= threshold]`\n",
    "  will return a list of columns that have missing values less than or equal to the threshold.\n",
    "- `data.dropna(subset =cols_to_drop, inplace = True )`\n",
    "\n",
    "\n",
    "     **Method 2** Impute by subgroups, the idea that different subgroups might have different characteristics, and filling missing values within these subgroups can lead to more accurate and meaningful data. Each subgroup if filled with something.\n",
    "\n",
    "\n",
    "```\n",
    "       new_variable = data.groupby(\"colwithsubgroups\")\n",
    "       [\"coltocal\"].mean().to_dict()\n",
    "```\n",
    "```\n",
    "        data[\"coltocal\"]data[\"coltocal\"].fillna\n",
    "        (data[\"colwithsubgroups\"].map(new_variable)\n",
    "```\n",
    " \n",
    "- In this method its smart to create a boxplot of columns with missing data to identify trents, outliers or just any other trends. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5cfe5c-5a63-4856-ab5b-982c2043c548",
   "metadata": {},
   "source": [
    "**2) Converting and analyzing categorical data**\n",
    "\n",
    "- looking for rows containing 1 phrase\n",
    "\n",
    "  \n",
    "  `data[\"column\"].str.contains(\"stslokkingfor\")`\n",
    "\n",
    "- Looking for rows containing several phrases using |\n",
    "\n",
    "  \n",
    "     `data[\"column\"].str.contains(\"stslokkingfor | another \")`\n",
    "    \n",
    "- Lokking for rows that starts with a specific phrase with ^\n",
    "\n",
    "\n",
    "     `data[\"column\"].str.contains(\"^stslokkingfor\")`\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "563ac15f-075a-486e-b26a-3a678f52ba07",
   "metadata": {},
   "source": [
    "# Create a list of categories\n",
    "flight_categories = [\"Short-haul\", \"Medium\", \"Long-haul\"]\n",
    "\n",
    "# Create short_flights 0-4 hours\n",
    "short_flights = \"^0h|^1h|^2h|^3h|^4h\"\n",
    "\n",
    "# Create medium_flights 5-9 hours\n",
    "medium_flights = \"^5h|^6h|^7h|^8h|^9h\"\n",
    "\n",
    "# Create long_flights 10-16 hours\n",
    "long_flights = \"10h|11h|12h|13h|14h|15h|16h\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "45c8452f-03c7-4448-ab1b-618a36611a35",
   "metadata": {},
   "source": [
    "# Create conditions for values in flight_categories to be created\n",
    "# Durationis an existing column in the data \n",
    "conditions = [\n",
    "    (planes[\"Duration\"].str.contains(short_flights)),\n",
    "    (planes[\"Duration\"].str.contains(medium_flights)),\n",
    "    (planes[\"Duration\"].str.contains(long_flights))\n",
    "]\n",
    "\n",
    "# Apply the conditions list to the flight_categories\n",
    "planes[\"Duration_Category\"] = np.select(conditions, \n",
    "                                        flight_categories,\n",
    "                                        default=\"Extreme duratio\")\n",
    "# Duration_Category is a new column added to the planes dataframe\n",
    "# The default value to assign when none of the conditions are met.\n",
    "\n",
    "# Plot the counts of each category\n",
    "sns.countplot(data=planes, x=\"Duration_Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8b9fe-7e65-4b6e-a8d4-fca9dd6be046",
   "metadata": {},
   "source": [
    "**3) Working with numeric data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5da5e-012e-498b-85dd-42d61f2b4073",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "        1- Remove comma values in the column        \n",
    "   `column.str.replace(\"original\", \"new\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3636b3-0bad-4f09-8513-640818d3ba33",
   "metadata": {},
   "source": [
    "        2- Convert the column to float         \n",
    "` data[\"column\"]= data[\"column\"].astype(float)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5286c3-171f-40a4-af58-0880e919c7b1",
   "metadata": {},
   "source": [
    "        3- create a new column by converting the currency?        \n",
    "  `data[\"new_col\"] = data[\"existing_col\"] * 0.012`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b60c2d-5ed1-482e-aac1-09399d177c6c",
   "metadata": {},
   "source": [
    "        4- Adding summary statistics into a DataFrame \n",
    "` data[\"std\"]= data.groupby(\"col\")[\"col\"].transform(lambda x: x.std())`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94587591-e4a1-4501-b26a-6ff047809c7f",
   "metadata": {},
   "source": [
    "**5) Dealing with Outliers**\n",
    "\n",
    "- 75th Percentile (Q3): The value below which 75% of the data falls\n",
    "\n",
    "`Q3 = data['values'].quantile(0.75)`\n",
    "- 25th Percentile (Q1): The value below which 25% of the data falls.\n",
    "\n",
    "`Q1 = data['values'].quantile(0.25)`\n",
    "\n",
    "- Then calculate IQR \n",
    "\n",
    "`IQR = Q3-Q1` \n",
    "\n",
    "- Finally calculate the Calculate the Lower Outlier Threshold\n",
    "\n",
    "`Lower_Outlier = Q1 - (1.5 * IQR)`\n",
    "\n",
    "- And the Upper Outlier Threshold\n",
    "\n",
    "`Upper_Outlier = Q3 + (1.5 * IQR)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560bdb4-9af6-4a0c-9fb9-fa7131b7070c",
   "metadata": {},
   "source": [
    "**Removing Outliers?**\n",
    "\n",
    "\n",
    "\n",
    "`no_outliers = data[(data[\"col\"] < Lower_Outlier) | (data[\"col\"] > Upper_Outlier) ] \\ [[\"col\", \"col\", \"col\"]]`\n",
    "\n",
    "\n",
    "`data= data.drop(no_outliers.index)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969dd42d-36ea-4051-ac47-968089815569",
   "metadata": {},
   "source": [
    "**6) Creating a PCA - principal component analysis**\n",
    "- it is a statistical technique used for dimensionality reduction. \n",
    "- technique used to simplify complex datasets by transforming the original features into a new set of features called principal components. These new features are uncorrelated with each other and ordered by the amount of variance they explain in the original data. It does focus more on the new features with high variance. \n",
    "\n",
    "   1. Make a new dataframe that only has the numeric columns. The index should be a label you want to maintain. \n",
    "            - Save the colum names on a new variable \n",
    "\n",
    "`columns = data.columns`\n",
    "        \n",
    "  2. scale the data with scale(). This will return an array not a datafrme. \n",
    "             - Standardizing data involves transforming your features so they have a mean of 0 and a standard deviation of 1. This is also known as Z-score normalization\n",
    "     \n",
    "   3. Make a dataframe from the array \n",
    "        \n",
    " `dataframe_from_array = pd.DataFrame(scaled_array, columns= original_columns)`\n",
    " \n",
    "   4. Check the mean of the new standardized dataframe\n",
    "  \n",
    "  `dataframe_from_array.mean()`\n",
    "  \n",
    "    -Checking the mean of the scaled features using the mean() DataFrame method is important because it helps you verify that the standardization process was done correctly.Remember the output will be in scientific notation. \n",
    "\n",
    "    5. check the mean of the new standardized dataframe. make sure to use ddof=0 parameter \n",
    "   \n",
    "   `dataframe_from_array.std()`\n",
    "   \n",
    "   6. Calculate the PCA transformation \n",
    "   \n",
    "   - The PCA model analyzes the scaled data (dataframe_from_array) and learns the principal components—essentially identifying the directions that capture the most variance in your data. This step doesn't change or transform your data; it just learns from it.\n",
    "   \n",
    "   `pca = PCA().fit(dataframe_from_array)`\n",
    "\n",
    "\n",
    "   7. Plot the cumulative variance ratio with number of components.\n",
    "\n",
    "- The explained_variance_ratio_ is an attribute provided by the PCA (Principal Component Analysis) object in the scikit-learn library. When you fit the PCA model to your data, it calculates how much of the total variance in the data is explained by each new feature created from original data. This information is stored in the explained_variance_ratio_ attribute.\n",
    "   \n",
    "   `plt.subplots(figsize=(10, 6))\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum())\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('label')\n",
    "plt.title(\"title\");`\n",
    "\n",
    "   8. Apply the transformation to the data to obtain the derived features.\n",
    "   \n",
    "   - The transform() method uses the learned principal components to project your original scaled data into the new principal component space. This creates a new representation of your data in terms of the principal components, effectively transforming your original data.\n",
    "   \n",
    "   ` transform_with_pca = pca.transform(dataframe_from_array)`\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
